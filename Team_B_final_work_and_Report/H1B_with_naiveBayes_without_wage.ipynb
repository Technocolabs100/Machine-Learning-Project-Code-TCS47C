#!/usr/bin/env python
# coding: utf-8

# In[1]:


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt 
import seaborn as sns 


# In[2]:


dataset = pd.read_csv('C:\my files\Machine_learning_internship\Main_project\H-1B_Disclosure_Data_FY2019.csv')


# In[3]:


dataset.head()


# In[4]:


dataset.shape


# # Selecting the features which will contribute prominently to the model building
# 

# In[5]:


dataset = dataset[['CASE_STATUS','VISA_CLASS','EMPLOYER_NAME','AGENT_REPRESENTING_EMPLOYER','SECONDARY_ENTITY_1','JOB_TITLE','SOC_TITLE','SOC_CODE', 'NAICS_CODE','CONTINUED_EMPLOYMENT', 'CHANGE_PREVIOUS_EMPLOYMENT','NEW_CONCURRENT_EMPLOYMENT', 'CHANGE_EMPLOYER','AMENDED_PETITION', 'H-1B_DEPENDENT', 'SUPPORT_H1B','WILLFUL_VIOLATOR','WAGE_RATE_OF_PAY_FROM_1','WAGE_RATE_OF_PAY_TO_1', 'WAGE_UNIT_OF_PAY_1','TOTAL_WORKER_POSITIONS','PREVAILING_WAGE_1']]


# # selecting only those data points for which the visa type is H1 - B

# In[6]:


dataset.CASE_STATUS.unique()


# In[7]:


dataset = dataset[((dataset['CASE_STATUS'].str.upper() == 'CERTIFIED') |                                (dataset['CASE_STATUS'].str.upper() == 'DENIED')) &                               (dataset['VISA_CLASS'].str.upper() == 'H-1B')] 


# In[8]:


dataset.CASE_STATUS.unique()


# # EDA

# In[9]:


dataset.CASE_STATUS.value_counts().plot(kind = 'bar')


# In[10]:


emp_name = dataset['EMPLOYER_NAME'].value_counts()
emp_name.to_frame()


# In[11]:


dataset['EMPLOYER_NAME'].value_counts()
sns.barplot(x = dataset['EMPLOYER_NAME'].value_counts()[:10], y = dataset['EMPLOYER_NAME'].value_counts().index[:10])


# In[12]:


sns.countplot(dataset.WILLFUL_VIOLATOR)


# In[13]:


dataset.AGENT_REPRESENTING_EMPLOYER.unique()


# In[14]:


sns.countplot(dataset.AGENT_REPRESENTING_EMPLOYER)


# In[15]:


dataset.JOB_TITLE.unique()


# In[16]:


dataset.head()


# In[17]:


dataset.info()


# In[18]:


dataset['H-1B_DEPENDENT'].head()


# In[19]:


dataset['H-1B_DEPENDENT'].unique()


# In[20]:


sns.countplot(dataset['H-1B_DEPENDENT'])


# In[21]:


ammendedPetition = dataset.AMENDED_PETITION
ammendedPetition.to_frame()


# In[22]:


dataset.CHANGE_EMPLOYER.head(30)


# In[23]:


dataset.CHANGE_EMPLOYER.unique()


# In[24]:


sns.countplot(dataset.CHANGE_EMPLOYER)


# In[25]:


dataset['SECONDARY_ENTITY_1'].head(50)


# In[26]:


dataset['SECONDARY_ENTITY_1'].unique()


# In[27]:


dataset['SECONDARY_ENTITY_1'].value_counts()


# In[28]:


sns.countplot(dataset['SECONDARY_ENTITY_1'])


# # Dealing with missing values

# In[29]:


dataset.isnull().sum()


# In[30]:


dataset.SUPPORT_H1B.value_counts()


# In[31]:


dataset.SUPPORT_H1B.describe()


# In[32]:


dataset.SUPPORT_H1B.fillna("Y",inplace = True)


# In[33]:


dataset.isnull().sum()


# In[34]:


dataset['SECONDARY_ENTITY_1'].describe()


# In[35]:


dataset['SECONDARY_ENTITY_1'].fillna("N",inplace = True)


# In[36]:


dataset.isnull().sum()


# In[37]:


dataset.SUPPORT_H1B.describe()


# In[38]:


dataset.SUPPORT_H1B.unique()


# In[39]:


sns.countplot(dataset.SUPPORT_H1B)


# In[40]:


dataset.SUPPORT_H1B.fillna("Y",inplace = True)


# In[41]:


sns.boxplot(dataset['WAGE_RATE_OF_PAY_FROM_1'])


# In[42]:


dataset['WAGE_RATE_OF_PAY_FROM_1'].describe()


# In[43]:


dataset['WAGE_RATE_OF_PAY_FROM_1'].isnull().sum()


# In[44]:


#sns.boxplot(dataset['WAGE_UNIT_OF_PAY_1'])
sns.countplot(dataset['WAGE_UNIT_OF_PAY_1'])


# In[45]:


(dataset['WAGE_UNIT_OF_PAY_1'])


# In[46]:


def convert(x):
  if x=='Y':
    return(0)
  else:
    return(1)


# In[47]:


dataset['WAGE_UNIT_OF_PAY_1'] = dataset['WAGE_UNIT_OF_PAY_1'].apply(convert)


# In[48]:


dataset['WAGE_UNIT_OF_PAY_1'].head(50)


# In[49]:


dataset['TOTAL_WORKER_POSITIONS'].value_counts()


# In[50]:


sns.boxplot(dataset['PREVAILING_WAGE_1'])


# In[51]:


plt.scatter(dataset['PREVAILING_WAGE_1'], dataset['CASE_STATUS'])


# In[52]:


dataset.pivot_table('PREVAILING_WAGE_1','CASE_STATUS').plot.bar()


# In[53]:


dataset['PREVAILING_WAGE_1'] = dataset['PREVAILING_WAGE_1'].clip(lower = dataset['PREVAILING_WAGE_1'].quantile(0.1), upper = dataset['PREVAILING_WAGE_1'].quantile(0.80))


# In[54]:


dataset['WAGE_RATE_OF_PAY_FROM_1'] = dataset['WAGE_RATE_OF_PAY_FROM_1'].clip(lower = dataset['WAGE_RATE_OF_PAY_FROM_1'].quantile(0.1), upper = dataset['WAGE_RATE_OF_PAY_FROM_1'].quantile(0.80))


# In[55]:


sns.boxplot(dataset['WAGE_RATE_OF_PAY_FROM_1'])


# In[56]:


dataset['WAGE_RATE_OF_PAY_FROM_1'].isnull().sum()


# In[57]:


sns.boxplot(dataset['PREVAILING_WAGE_1'])


# In[58]:


dataset['PREVAILING_WAGE_1'].count()


# In[59]:


dataset['PREVAILING_WAGE_1'].isnull().value_counts()


# In[60]:


dataset['PREVAILING_WAGE_1'].describe()


# In[61]:


dataset['PREVAILING_WAGE_1'] = dataset['PREVAILING_WAGE_1'].fillna(dataset['PREVAILING_WAGE_1'].mean())


# In[62]:


dataset['PREVAILING_WAGE_1'].isnull().value_counts()


# # dropping the null values of EMPLOYER_NAME,                  AGENT_REPRESENTING_EMPLOYER, SOC_TITLE, SOC_CODE,                  NAICS_CODE 

# In[63]:



dataset.dropna(axis = 0, inplace =True)


# In[64]:


dataset.isnull().sum()


# # transforming the Y and N into 0 and 1

# In[65]:


def transformation(x):
  if x=='Y':
    return(0)
  else:
    return(1)


# In[66]:


dataset['SUPPORT_H1B'] = dataset['SUPPORT_H1B'].apply(transformation)


# In[67]:


dataset.SUPPORT_H1B.value_counts()


# In[68]:


dataset['SECONDARY_ENTITY_1'] = dataset['SECONDARY_ENTITY_1'].apply(transformation)


# In[69]:


dataset['SECONDARY_ENTITY_1'].value_counts()


# In[70]:


dataset['H-1B_DEPENDENT'] = dataset['H-1B_DEPENDENT'].apply(transformation)


# In[71]:


dataset['H-1B_DEPENDENT'].unique()


# In[72]:


dataset['WILLFUL_VIOLATOR'] = dataset['WILLFUL_VIOLATOR'].apply(transformation)


# In[73]:


dataset['WILLFUL_VIOLATOR'].unique()


# In[74]:


dataset['WILLFUL_VIOLATOR'].value_counts()


# In[75]:


dataset['AGENT_REPRESENTING_EMPLOYER'].unique()


# In[76]:


dataset['AGENT_REPRESENTING_EMPLOYER'] = dataset['AGENT_REPRESENTING_EMPLOYER'].apply(transformation)


# In[77]:


dataset['AGENT_REPRESENTING_EMPLOYER'].value_counts()


# # checking with the JOB_TITLE,	SOC_TITLE,	SOC_CODE,	NAICS_CODE features

# In[78]:


dataset['JOB_TITLE'].value_counts()
sns.barplot(x = dataset['JOB_TITLE'].value_counts()[:10], y = dataset['JOB_TITLE'].value_counts().index[:10])


# In[79]:


dataset['JOB_TITLE'].nunique()


# In[80]:


dataset['SOC_TITLE'].value_counts()
sns.barplot(x = dataset['SOC_TITLE'].value_counts()[:10], y = dataset['SOC_TITLE'].value_counts().index[:10])


# In[81]:


dataset['SOC_CODE'].value_counts()
sns.barplot(x = dataset['SOC_CODE'].value_counts()[:10], y = dataset['SOC_CODE'].value_counts().index[:10])


# In[82]:


dataset['NAICS_CODE'].value_counts()


# In[83]:


dataset['NAICS_CODE'].nunique()


# In[84]:


dataset['SOC_CODE'].nunique()


# In[85]:


#dataset['JOB_TITLE'] = dataset['JOB_TITLE'].str.replace("%" , "").astype(float)
#dataset.loc[dataset['JOB_TITLE']=='ADVERSTING AND PROMOTIONS MANAGER']


# In[86]:


#dataset['SUPPORT_H1B'].mode().values[0]


# # using label encoding to convert string data points of a variable into numeric data 

# In[87]:


from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
label = le.fit_transform(dataset['CASE_STATUS'])
label


# In[88]:


dataset.drop("CASE_STATUS", axis=1, inplace=True)
dataset["CASE_STATUS"] = label
dataset["CASE_STATUS"].head(30)


# In[89]:


import sys
dataset['SOC_TITLE_NEW'] = 'others'
dataset['SOC_TITLE_NEW'][dataset['SOC_TITLE'].str.contains('WEB|SOFTWARE|COMPUTER|INFORMATION|SECURITY')] = 'IT ENGINEERS'
dataset['SOC_TITLE_NEW'][dataset['SOC_TITLE'].str.contains('MECHANICAL|DESIGN')] = 'MECHANICAL'
dataset['SOC_TITLE_NEW'][dataset['SOC_TITLE'].str.contains('CHIEF|EXECUTIVES')] = 'Executives'
dataset['SOC_TITLE_NEW'][dataset['SOC_TITLE'].str.contains('Chief|MANAGEMENT|MANAGERS')] = 'Manager'
dataset['SOC_TITLE_NEW'][dataset['SOC_TITLE'].str.contains('CHEMICAL|MARINE|INDUSTRIAL|MATERIALS')] = 'MULTIDISCPLINARY ENGINEERS'
dataset['SOC_TITLE_NEW'][dataset['SOC_TITLE'].str.contains('DATA|Database|STATISTICIANS')] = 'Database & Scientists'
dataset['SOC_TITLE_NEW'][dataset['SOC_TITLE'].str.contains('Sales|Market')] = 'Sales & Market'
dataset['SOC_TITLE_NEW'][dataset['SOC_TITLE'].str.contains('FINANCIAL|ECONOMISTS')] = 'Finance'
dataset['SOC_TITLE_NEW'][dataset['SOC_TITLE'].str.contains('COMPLIANCE|PUBLIC RELATIONS|Fundraising')] = 'P.R'
dataset['SOC_TITLE_NEW'][dataset['SOC_TITLE'].str.contains('education|law')] = 'Administrative'
dataset['SOC_TITLE_NEW'][dataset['SOC_TITLE'].str.contains('ACCOUNTANTS|Auditors|Compliance')] = 'Audit'
#H1B_visa['SOC_TITLE_NEW'][H1B_visa['SOC_TITLE'].str.contains('Recruiters|HUMAN RESOURCES|')] = 'H.R'
dataset['SOC_TITLE_NEW'][dataset['SOC_TITLE'].str.contains('Agricultural|Farm')] = 'Agriculture'
dataset['SOC_TITLE_NEW'][dataset['SOC_TITLE'].str.contains('Construction|Architectural')] = 'Estate'
dataset['SOC_TITLE_NEW'][dataset['SOC_TITLE'].str.contains('INTERNISTS|DENTISTS|THERAPISTS|SURGEONS|BIOMEDICAL')] = 'Medical'
dataset['SOC_TITLE_NEW'][dataset['SOC_TITLE'].str.contains('WRITERS|TEACHERS|POSTSECONDARY|KINDERGARTEN AND ELEMENTARY SCHOOL')] = 'Education'
dataset['SOC_TITLE_NEW'][dataset['SOC_TITLE'].str.contains('TECHNICIANS|WORKERS|CHEMISTS|BIOCHEMISTS')] = 'TECHNICIANS'


# In[90]:


dataset.head()
    


# In[91]:


import sys
dataset['JOB_TITLE_NEW'] = 'others'
dataset['JOB_TITLE_NEW'][dataset['JOB_TITLE'].str.contains('IOS|DEVOPS|CLOUD|FRONT END|INTERIOR|.NET|DEVOPS|SOFTWARE|COMPUTER|INFORMATION|SECURITY|SYSTEMS|AUTOMATION|SYSTEMS|FULL STACK|LEAD|JAVA|IT|TEST|GRAPHIC|SUPPORT')] = 'IT & SOFTWARE ENGINEERS'
dataset['JOB_TITLE_NEW'][dataset['JOB_TITLE'].str.contains('QA|ENGAGEMENT|OPERATIONS|DELIVERY|INFRASTRUCTURE|FIRMWARE|ANDRIOD|UX|RF|PYTHON|TABLEAU|HADOOP|INFORMATICA|SQL|BI|SCRUM|VALIDATION|APPLICATIONS|UI|PROGRAMMER|DEVELOPER|SOLUTION|RPA')] = 'IT & SOFTWARE ENGINEERS'
dataset['JOB_TITLE_NEW'][dataset['JOB_TITLE'].str.contains('LANDSCAPE|CAD|SITE|FIELD|QUALITY|MECHANICAL DESIGN|STRUCTURAL|DESIGNER|SIMULATION|ENGINEERING|MARINE|INDUSTRIAL|MATERIALS|MECHANICAL|MANUFACTURING|CIVIL')] = 'MECHANICAL & CIVIL ENGINEER '
dataset['JOB_TITLE_NEW'][dataset['JOB_TITLE'].str.contains('ACCOUNTANT|FINANCIAL|QUANTITATIVE|RISK|BUDGET|TAX')] = 'FINANCE TEAM'
dataset['JOB_TITLE_NEW'][dataset['JOB_TITLE'].str.contains('PRESIDENT|DIRECTOR|MANAGER')] = 'Manager & DIRECTORS'
#H1B_visa['JOB_TITLE_NEW'][H1B_visa['JOB_TITLE'].str.contains('ELECTRICAL|CHEMICAL')] = 'ELECTRICAL ENGINEERS'
dataset['JOB_TITLE_NEW'][dataset['JOB_TITLE'].str.contains('SERVICE|AEM|EMBEDDED|DIGITAL|NETWORK|CONTROLS|HARDWARE|FUNCTIONAL|ELECTRICAL|CHEMICAL')] = 'ELECTRONICS & ELECTRONICS ENGINEERS TEAM'
dataset['JOB_TITLE_NEW'][dataset['JOB_TITLE'].str.contains('PUBLIC|LAWYERS|ATTORNEY|LAW')] = 'LAW TEAM'
dataset['JOB_TITLE_NEW'][dataset['JOB_TITLE'].str.contains('SALESFORCE|MARKET|MARKETING|SUPPLY')] = 'MARKETING TEAM'
dataset['JOB_TITLE_NEW'][dataset['JOB_TITLE'].str.contains('SPEECH|BIG|ORACLE|MACHINE|DATABASE|DATA|SCIENTIST|ASSOCIATES')] = 'DATABASE & SCIENTISTS'
dataset['JOB_TITLE_NEW'][dataset['JOB_TITLE'].str.contains('ARCHITECT|ARCHITECTURAL')] = 'ARCHITECT'
dataset['JOB_TITLE_NEW'][dataset['JOB_TITLE'].str.contains('TEACHER|PROFESSOR|POSTDOCTORAL|FELLOW|SCHOLAR|LECTURER|LABORATORY')] = 'EDUCATIONAL ORGANISATION'
dataset['JOB_TITLE_NEW'][dataset['JOB_TITLE'].str.contains('BUSINESS|ADMINISTRATOR|INVESTMENT|ACCOUNT')] = 'BUSINESS TEAM'
dataset['JOB_TITLE_NEW'][dataset['JOB_TITLE'].str.contains('DENTIST|HOSPITALIST|THERAPIST|PSYCHIATRIST|PEDIATRICIAN|PHYSICIAN|FAMILY|NEPHROLOGIST')] = 'MEDICAL TEAM'
dataset['JOB_TITLE_NEW'][dataset['JOB_TITLE'].str.contains('SENIOR|SR.|SR')] = 'SENIOR TEAM'


# In[92]:


dataset['SOC_CODE'] = dataset['SOC_CODE'].replace(['OPERATIONS RESEARCH ANALYSTS'],'15')
dataset['SOC_CODE_NEW'] = dataset['SOC_CODE'].str.split("-").str[0]
dataset['SOC_CODE_NEW'] = dataset['SOC_CODE_NEW'].replace(['39','35','53','51','47','49','31','33','45','37'],'1000')#'10 codes lessthan 100')
dataset['SOC_CODE_NEW'].value_counts()


# In[93]:


dataset['CONTINUED_EMPLOYMENT'] = dataset['CONTINUED_EMPLOYMENT'].replace(['001','01'],'1')
dataset['CONTINUED_EMPLOYMENT'] = dataset['CONTINUED_EMPLOYMENT'].replace(['00'],'0')
dataset['CONTINUED_EMPLOYMENT'] = dataset['CONTINUED_EMPLOYMENT'].replace(['02'],'2')
dataset['CONTINUED_EMPLOYMENT'] = dataset['CONTINUED_EMPLOYMENT'].replace(['B'],'1')
dataset['CONTINUED_EMPLOYMENT'] = dataset['CONTINUED_EMPLOYMENT'].replace(['0',],'1')
dataset['CONTINUED_EMPLOYMENT'] = dataset['CONTINUED_EMPLOYMENT'].replace(['25','20','15','6','8','12','30','50','40','18','35','13','7','99','45','17','21','11'],'lower values 100 frequency')
dataset['CONTINUED_EMPLOYMENT'].value_counts()


# In[94]:


dataset['NAICS_CODE_NEW'] = dataset['NAICS_CODE'].astype(str).str[0:2]
dataset['NAICS_CODE_NEW'].value_counts()


# In[95]:


import sys
dataset['EMPLOYER_BRANCH'] = 'others'
dataset['EMPLOYER_BRANCH'][dataset['EMPLOYER_NAME'].str.contains('APPLE|GOOGLE|FACEBOOK|CAPGEMINI|WIPRO|TWITTER|INFOSYS|MICROSOFT|AIRLINES|IBM|ERNST|JPMORGAN|MINDTREE|AMAZON|TATA')] = 'TOP TECH'
dataset['EMPLOYER_BRANCH'][dataset['EMPLOYER_NAME'].str.contains('ELECTRONIC|MARIX|MICRO|ELECTRO|CHIP|DEVICE|INSTRUMENTS|INTEGRATORS|DELL|HEW|SEMICONDUCTORS|ENTERTAINMENT|LOGIC')] = 'ELECTRONIC & LOGISTICS SERVICES'
dataset['EMPLOYER_BRANCH'][dataset['EMPLOYER_NAME'].str.contains('UNIVERSITY|UNIVERSITIES|ACADEMIC|INSTITUTIONS|SCIENCE|NATIONAL|SCHOOL')] = 'UNIVERSITY'
dataset['EMPLOYER_BRANCH'][dataset['EMPLOYER_NAME'].str.contains('MASTER|BANK|CARD|VISA')] = 'BANKING COMPANIES'
dataset['EMPLOYER_BRANCH'][dataset['EMPLOYER_NAME'].str.contains('HEALTH|FIN|ECLINICALWORKS|MEDTRONIC|FINANCIAL|MEDICAL|MED|CENTER')] = 'FINANCE AND MEDICAL SOLUTIONS'
dataset['EMPLOYER_BRANCH'][dataset['EMPLOYER_NAME'].str.contains('BUSINESS|MANAGEMENT')] = 'BUSINESS SOLUTIONS'
dataset['EMPLOYER_BRANCH'][dataset['EMPLOYER_NAME'].str.contains('LABS|COMMUNICATION|NETWORK|DIGITAL|NETWORKS')] = 'RESEARCH LABS & NETWORK'
dataset['EMPLOYER_BRANCH'][dataset['EMPLOYER_NAME'].str.contains('AUTOBILE|AUTOMOTIVE|MOTOR|AUTO|FORD|PUMP|ELECTRIC|TESLA|BOSCH')] = 'AUTOMOTIVE & ELECTRICAL'
dataset['EMPLOYER_BRANCH'][dataset['EMPLOYER_NAME'].str.contains('DEVELOPMENT|IT|COMPUTER|CYBER|TECHNOLOGY|TECH|SOLUTIONS|WEB|INFOTECH|CLOUD|VISION|GLOBAL|SYSTEMS|TECHNOSOFT|TECHNO|SERVICES|SECURITIES|SECURITY|TECHNOLOGIES|DATA')] = 'TECH SOLUTIONS'
dataset['EMPLOYER_BRANCH'][dataset['EMPLOYER_NAME'].str.contains('INTERNATIONAL|CONSULTING|CONSULTANT|RESOURCES|GROUP|ASSOCIATES|ANALYSTS')] = 'CONSULTING COMPANIES'
dataset['EMPLOYER_BRANCH'][dataset['EMPLOYER_NAME'].str.contains('PRODUCT|PRODUCTS|ENTERPRISE|ENTERPRISES')] = 'PRODUCT &ENTERPRISE COMPANIES'


# In[96]:


dataset.head()


#  
# 
# dataset.drop('EMPLOYER_BRANCH', axis=1, inplace=True) 
# dataset.drop('SOC_TITLE_NEW', axis=1, inplace=True) 
# dataset.drop('JOB_TITLE_NEW', axis=1, inplace=True) 
# 
# dataset.drop('CONTINUED_EMPLOYMENT', axis=1, inplace=True) 
# dataset.drop('SOC_CODE_NEW', axis=1, inplace=True)
# dataset.drop('NAICS_CODE_NEW', axis=1, inplace=True)

# In[97]:


label1 = dataset[['SOC_TITLE_NEW']].copy() #Create an extra dataframe which will be used to address only the encoded values
label1['SOC_TITLE_NEW_ENCODED'] = le.fit_transform(dataset['SOC_TITLE_NEW'].values)


# In[98]:


data = label1.drop_duplicates('SOC_TITLE_NEW_ENCODED')
data


# In[99]:


dataset['SOC_TITLE_NEW'].unique()


# In[100]:


le_socTitle = le.fit_transform(dataset['SOC_TITLE_NEW'])
dataset.drop("SOC_TITLE_NEW", axis=1, inplace=True)
dataset["SOC_TITLE_NEW"] = le_socTitle
print(dataset["SOC_TITLE_NEW"].unique())


# In[101]:


dataset.head(30)


# In[102]:


dataset["NAICS_CODE_NEW"].head()


# In[103]:


label2 = dataset[['EMPLOYER_BRANCH']].copy() 
label2['EMPLOYER_BRANCH_ENCODED'] = le.fit_transform(dataset['EMPLOYER_BRANCH'].values)
data2 = label2.drop_duplicates('EMPLOYER_BRANCH_ENCODED')
data2


# In[104]:


le_employer = le.fit_transform(dataset['EMPLOYER_BRANCH'])
dataset.drop("EMPLOYER_BRANCH", axis=1, inplace=True)
dataset["EMPLOYER_BRANCH"] = le_employer
dataset["EMPLOYER_BRANCH"].head(30)


# In[105]:


label3 = dataset[['JOB_TITLE_NEW']].copy() 
label3['JOB_TITLE_NEW_ENCODED'] = le.fit_transform(dataset['JOB_TITLE_NEW'].values)
data3 = label3.drop_duplicates('JOB_TITLE_NEW')
data3


# In[106]:


le_jobt = le.fit_transform(dataset['JOB_TITLE_NEW'])
dataset.drop("JOB_TITLE_NEW", axis=1, inplace=True)
dataset["JOB_TITLE_NEW"] = le_jobt
dataset["JOB_TITLE_NEW"].head(30)


# In[107]:


dataset.head()


# In[108]:


dataset = dataset.drop('EMPLOYER_NAME', axis = 1)
dataset = dataset.drop('SOC_TITLE', axis = 1)
#dataset = dataset.drop('SOC_CODE', axis = 1)
#dataset = dataset.drop('JOB_TITLE', axis = 1)
#dataset = dataset.drop('NAICS_CODE', axis = 1)


# In[109]:


dataset.corr()


# In[110]:


dataset = dataset.drop('WAGE_UNIT_OF_PAY_1', axis = 1)


# In[111]:


sns.heatmap(dataset.corr())


# # converting string dtype to numeric

# In[112]:


dataset[['SOC_CODE_NEW','NAICS_CODE_NEW']] = dataset[['SOC_CODE_NEW','NAICS_CODE_NEW']].apply(pd.to_numeric)


# In[113]:


dataset[['SOC_CODE_NEW']] = dataset[['SOC_CODE_NEW']].astype('int32') 
dataset[['NAICS_CODE_NEW']] = dataset[['NAICS_CODE_NEW']].astype('int32')


# In[114]:


dataset.dtypes


# # segregating the x and y features

# In[133]:


x_features = dataset[['SOC_TITLE_NEW','EMPLOYER_BRANCH','JOB_TITLE_NEW','SOC_CODE_NEW','NAICS_CODE_NEW']]


# In[134]:


y_features = dataset[['CASE_STATUS']]


# In[135]:


from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(x_features, y_features, test_size = 0.3, random_state = 0)


# In[136]:


#from sklearn.preprocessing import MinMaxScaler
#sc = MinMaxScaler()
#X_train = sc.fit_transform(X_train) 
#X_test = sc.transform(X_test)


# # modelling part

# In[137]:


from sklearn.naive_bayes import MultinomialNB
clf = MultinomialNB()
clf.fit(X_train, y_train)


# In[138]:


predictions = clf.predict(X_test)


# In[139]:


from sklearn.metrics import confusion_matrix, classification_report
print(classification_report(y_test, predictions))


# In[140]:


from imblearn.over_sampling import SMOTE
sm = SMOTE(random_state = 42)
X_train_res, y_train_res = sm.fit_resample(X_train, y_train)


# In[141]:


clf1 = MultinomialNB()
clf1.fit(X_train_res, y_train_res)


# In[142]:


predictions1 = clf1.predict(X_test)


# In[143]:


print(classification_report(y_test, predictions1))


# In[144]:


from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.metrics import r2_score
cm = confusion_matrix(y_test,predictions)
cm1 = confusion_matrix(y_test,predictions1)


# In[145]:


print("Confusion Matrix before resampling: ")
print(cm)
print("r2 before resampling: {}".format(r2_score(y_test, predictions)))
print("Accuracy of the Model before resampling: {0}%".format(accuracy_score(y_test, predictions)*100))
rsquared = clf.score(X_test, y_test)
print('score before resampling',rsquared)

#adjusted r2
n=len(dataset) 
p=len(dataset.columns)-1 
adjr= 1-(1-rsquared)*(n-1)/(n-p-1)
print('adjusted score after resampling',adjr)


# In[146]:


print("Confusion Matrix after resampling: ")
print(cm1)
print("r2 after resampling: {}".format(r2_score(y_test, predictions1)))
print("Accuracy of the Model after resampling: {0}%".format(accuracy_score(y_test, predictions1)*100))

rsquared1 = clf1.score(X_test, y_test)
print('score after resampling',rsquared1)

#adjusted r2
n=len(dataset) 
p=len(dataset.columns)-1 
adjr1= 1-(1-rsquared1)*(n-1)/(n-p-1)
print('adjusted score after resampling',adjr1)

from sklearn.metrics import roc_auc_score, roc_curve,f1_score, precision_score
import matplotlib.pyplot as plt

nsProbability = [0 for _ in range(len(y_test))]
lsProbability = clf1.predict_proba(X_test)
# keep probabilities for the positive outcome only
lsProbability = lsProbability[:, 1]
# calculate scores
nsAUC = roc_auc_score(y_test, nsProbability)
lrAUC1 = roc_auc_score(y_test, lsProbability)
# summarize scores
print('No Skill: ROC AUC=%.3f' % (nsAUC*100))
print('naive bayes Skill: ROC AUC=%.3f' % (lrAUC1*100))
# calculate roc curves
nsFP, nsTP, _ = roc_curve(y_test, nsProbability)
lrFP, lrTP, _ = roc_curve(y_test, lsProbability)
# plot the roc curve for the model
plt.plot(nsFP, nsTP, linestyle='--', label='No Skill')
plt.plot(lrFP, lrTP, marker='*', label='naive bayes')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
# show the legend
plt.legend()
plt.show()
# # making a predictive system
SOC_TITLE_NEW = int(input('enter the soc title:'))
EMPLOYER_BRANCH = int(input('enter the enterprise type:'))
JOB_TITLE_NEW = int(input('enter the job title:'))
SOC_CODE_NEW = int(input('enter the first two digits of soc code:'))
NAICS_CODE_NEW = int(input('enter the naics code:'))
# In[147]:


input_data = (8,11,1,11,54)
input_data_as_numpy_array = np.asarray(input_data)
Visa = input_data_as_numpy_array.reshape(1,-1)


# In[148]:


#Visa = [[SOC_TITLE_NEW, EMPLOYER_BRANCH, JOB_TITLE_NEW, SOC_CODE_NEW, NAICS_CODE_NEW]]
result = clf1.predict(Visa)


# In[149]:


print(result)
if (result[0] == 0):
    print('The person can be granted H1B visa')
else:
    print('Sorry, but the person will not be granted H1B visa')
         


# # Saving the trained model

# In[150]:


import pickle


# In[151]:


filename =  'trained model.sav'
pickle.dump(clf1 , open(filename, 'wb'))


# # loading the saved model

# In[152]:


loaded_model = pickle.load(open('trained model.sav','rb'))


# In[153]:


#Visa = [[SOC_TITLE_NEW, EMPLOYER_BRANCH, JOB_TITLE_NEW, SOC_CODE_NEW, NAICS_CODE_NEW]]
input_data = (8,11,1,11,54)
input_data_as_numpy_array = np.asarray(input_data)
Visa = input_data_as_numpy_array.reshape(1,-1)
result = loaded_model.predict(Visa)

print(result)
if (result[0] == 0):
    print('The person can be granted H1B visa')
else:
    print('Sorry, but the person will not be granted H1B visa')


# In[ ]:





# In[ ]:




